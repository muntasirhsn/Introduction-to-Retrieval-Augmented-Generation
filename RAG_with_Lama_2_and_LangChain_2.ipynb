{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"    
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG with Llama 2 and LangChain\n",
        "Retrieval-Augmented Generation (RAG) is a technique that combines a retriever and a generative language model to deliver accurate response. It involves retrieving relevant information from a large corpus and then generating contextually appropriate responses to queries. Here we use the quantized version of the Llama 2 13B LLM with LangChain to perform generative QA with RAG. The notebook file has been tested in Google Colab with T4 GPU. Please change the runtime type to T4 GPU before running the notebook."
      ],
      "metadata": {
        "id": "ce4-y4JJgVSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "tPpiHVgj4CmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install sentence_transformers # ==2.2.2\n",
        "!pip install unstructured\n",
        "!pip install pdf2image\n",
        "!pip install pdfminer.six\n",
        "!pip install unstructured-pytesseract\n",
        "!pip install unstructured-inference\n",
        "!pip install faiss-gpu\n",
        "!pip install pikepdf\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "zjLkm65J_S0v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart Runtime"
      ],
      "metadata": {
        "id": "30AIKqF9rTuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Llama 2\n",
        "We will use the quantized version of the LLAMA 2 13B model from HuggingFace for our RAG task."
      ],
      "metadata": {
        "id": "X22Ccat1oXE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "model_name = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_hQ_IfuoWT6",
        "outputId": "f1f88175-c88e-4bfb-d89c-b47cdf1d1ba6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test LLM with Llama 2 prompt structure and LangChain PromptTemplate"
      ],
      "metadata": {
        "id": "R69LfWsnYAqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are an AI assistant. You are truthful, unbiased and honest in your response.\n",
        "\n",
        "If you are unsure about an answer, truthfully say \"I don't know\"\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "text = \"Explain artificial intelligence in a few lines\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "id": "BX6bnnL45a2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ad731e-0310-4ecf-b940-f4a02ef15e6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here is my explanation of artificial intelligence:  Artificial intelligence (AI) refers to the\n",
            "development of computer systems that can perform tasks that typically require human intelligence,\n",
            "such as learning, problem-solving, and decision-making. These systems use algorithms and machine\n",
            "learning techniques to analyze data and make predictions or take actions based on that data. Some\n",
            "examples of AI include natural language processing, image recognition, and autonomous vehicles.\n",
            "Overall, the goal of AI is to create machines that can think and act like humans, but with greater\n",
            "speed, accuracy, and consistency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "M-BEF0Zgbsrf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG from web pages\n",
        "### A. Create a vectore store for the context/external data\n",
        "Here, we'll create embedding vectores of the unstructured data loaded from the the source and store them in a vectore store.  "
      ],
      "metadata": {
        "id": "a2SNJZEtlDo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Load the document\n",
        "\n",
        "Depending on the type of the source data, we can use the appropriate data loader from LangChain to load the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ffMdNNqecUq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "\n",
        "web_loader = UnstructuredURLLoader(\n",
        "    urls=[\"https://en.wikipedia.org/wiki/Solar_System\"], mode=\"elements\", strategy=\"fast\",\n",
        "    )\n",
        "web_doc = web_loader.load()\n",
        "updated_web_doc = filter_complex_metadata(web_doc)"
      ],
      "metadata": {
        "id": "z8Y3YnRjJGEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0cf5f4e-6ff3-45cf-9312-c006871bbfd6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Split the documents into chunks\n",
        "\n",
        "Due to the limited size of the context window of an LLM, the data need to be divided into smaller chunks with a text splitter like ``CharacterTextSplitter`` or ``RecursiveCharacterTextSplitter``. In this way, the smaller chunks can be fed into the LLM."
      ],
      "metadata": {
        "id": "5rS8wix4cnKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
        "chunked_web_doc = text_splitter.split_documents(updated_web_doc)\n",
        "len(chunked_web_doc)"
      ],
      "metadata": {
        "id": "GVjSIuPMmYvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd755d0-5cbb-4d08-fe20-487c655901b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1452"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a vector database of the chunked documents with HuggingFace embeddings"
      ],
      "metadata": {
        "id": "k03r4pKIcyAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "d4fa63da64fe4ef9a95983c33e81b3fb",
            "81db09a97bb64ddca5ced75c88a58506",
            "da82509ce2ba4e65ba650287eb63eed5",
            "59278e859ba14704919210b84bdcc350",
            "54937ac6f94943b8a75e9cfef55232e0",
            "a476050e214b4b1d9d98d6c8074e924f",
            "9baeeb8ac5614e90949215edaa217cde",
            "9c68e78d615c45b3ad36f59622946da3",
            "7efe5d6195224258817ec1f8e686d01e",
            "5a92e261b7e94f7683109254a8053fa0",
            "fecb2397fd604fd3ad471eee8d6b40d4",
            "5858b0df306a450681349d2fd293cb7a",
            "8c043e67e50e4057ba4e1081bd6666ae",
            "cbea64c01e5c4be7a5e08dc0eb27cea5",
            "d83c1777836746329fb42902a8a9d5dc",
            "355f57d4af3c4f6c9d622367af9e4ae8",
            "5be0c6b84fad4bed8599de16c069cc34",
            "0165c302d7084795bdcb36608451124d",
            "faa485cd691143e59341fb2e15e8244d",
            "71ba5c4c8a14445ca272356e69ad3403",
            "c80ca633d7cd495c993dbb9197556643",
            "9a27999390c146f6806f13045cf093de",
            "6067747070564704a1517353e1d84202",
            "cbc725a3cfb64ba1807fb49ff547c443",
            "5d6e36f9ea114e5797cbd0b0746a183e",
            "403099bc30544486b8dc87a540d4531d",
            "a447539726964f0f81a1b8b73059ed74",
            "d541f34fd003494cb923f0286f566990",
            "1cd539378b194751bae8e921bfdbccc3",
            "fd1c29177b54481ea7cb6c16d9a4dbeb",
            "c6502f9fffad4b0fb5b4404ea775111a",
            "6fb02cdef05e41a0af5f9ab68ce92409",
            "53f3136bfe344f899e6afee53f3de55e",
            "dca8f9e656a042bebb3ef4fe1a2449d6",
            "a62a941cbfa84ddfabc6412ed3e6d803",
            "cb350281d55a44148e17b0f1ae5d97d9",
            "601d95151baa442e808aea4529331b9b",
            "2989e7a917d04efa87029c682d1d5d13",
            "d7662f2ef45840b39fe0c993128ff360",
            "afcaba837d65451e9f1542a3ed5d3a21",
            "f992cabfd8c4455fbc33d707368df725",
            "004dd6c7d1a84e3f9a66c0160a220a59",
            "016133a221974c2e9af53c95df5ac705",
            "6e852597aa5947eca1bd51fb90f60350",
            "ef4e65803bb44723b2333ee48e6d37b5",
            "931423de2cf94a4ca22eab89cb61a752",
            "c9a7ed88b6554af2b2644d2d603c71f3",
            "390ad1d52f064711851be0be875e7e92",
            "909f91958b034c5a9c319d1e6c968a0f",
            "57bf8ddcafd147a489f626be902eb7cc",
            "f29056bed7ac4201b08a900e89233c46",
            "d62eb707bb2d40218c1b1ef8973951b9",
            "b8a0933cbb1646f885bd49175546c38c",
            "c45b9b4fe96d4cfaab6fdb1f53670146",
            "57b7c2de993e42568333667492b0b39b",
            "ad76b76193d54428b21df0a45f13cc03",
            "a9ce0bb1938044088ad8bbb8767d057f",
            "f098b02880174173b6ac7e9c44e0303f",
            "da30f7a885a14c1b985786a4f579e7e2",
            "203668ca2e0646a5a2e83fc0896d6483",
            "f8be35974a454aaabfac2afeccfbcb5a",
            "ad37009f6cfa445e9dfea3de2e425fe3",
            "5b830e4c2bec4fe391cf8bd04f3d9b83",
            "e9e2f2c4ef884f4c96469818f384eb2e",
            "215576d0e47e46a8aeb65ce380b450a1",
            "1b62e99721b345aa92507d3282c9b4ca",
            "4f2c94e7d34840f2900a0ab79c77ac4a",
            "38fd157ea95541ec9fcf55172d1bcf4f",
            "fbc482ed8efe4236b45165ff4eb367c3",
            "278e99da78f8433cbe5c13888bf336af",
            "c23d31de5d5c4c43b51c543725925c67",
            "1ed45086f8cf46c5974ae9d5702ba328",
            "aeab70a1e4c640d49ed854bb70be5955",
            "cf8dc66c840c4de1a84a39d2713f92bc",
            "94f554c55d274b9aa75ca7190b12dd0b",
            "21a72a145a374ad7b750f0a6efa446c4",
            "e510d2c424a444898af58d2229008fb6",
            "e7f4d506ec334e2484a04375eb52ec3b",
            "08c8ef9f68374707a128cdcd5cfe83f1",
            "6aa1715fc6c3431bbe0790c1dbc3abc1",
            "4586037efb7645caaf926058a18e87d6",
            "9ce5cb65977c41859c2d418387b0d9a2",
            "62b3a58ef5e947b496f73af817ff9786",
            "c5fb481be6bf458aa64b23cd17aaaf56",
            "742d52a37a924f28a2708c427f9c8986",
            "d7d324cd815f4110ba4cd413ec673485",
            "660203a607734ec6a0215fd6e4aff2b1",
            "2806106a276c4615acbfd4a1b727b023",
            "25482ba69e274aec8bf2022971244f5c",
            "a44348dafcba4c3c8bb8ac0f520c78cd",
            "dbc12d3f921b4d018b8cadc59a90aac8",
            "9f66d29db13a4dd8a6b47b30cff449aa",
            "bfd9effcdfa24252863b806c2552fddd",
            "443ddaf039b14e28879fdc97d8350aaa",
            "e876938812b449d7b2a6f86a78018962",
            "c4ac3cd12a3242e496a7c7d52325037b",
            "215e877d137746cda8b924a645355fdf",
            "030d4a3dd95a4b148f5da3b1b138c9ce",
            "8d894a10ee434a338506e71296d3850b",
            "fa1fc5a322d446dcbb8865891693db86",
            "92dcd6c1bef74e11a6799a836dae9ed6",
            "5250abc63a9d4eceb30a9f448fd0b2a2",
            "de1ec7d9f1f54ae2a59f10090e641082",
            "1cab63c815cb4ebc895e9e7341f6b83b",
            "c857fb0076174a2d9a223003177c4306",
            "481614f6b3ab4185bb3d9e308822c7f3",
            "20e86acdab4546959385032a21e379cb",
            "95531ca8c825488bb67265e353cfb4ea",
            "d975c3162e2947c392108896a79228b9",
            "79b3f9c90cb04bf0993e25d64922c7c4",
            "2cb169c90e1d436da5e1632b00551936",
            "f0f75f5edd5c4a879fc578bad605214f",
            "0d9c69eaa009443bb008a00ab5514036",
            "ef8044f573bf4675be7e2929193fe8ec",
            "4c54695b1abf46acbdbd649a69077543",
            "f74c37d39bb6426ea17692f1f6fd1a29",
            "0abc1451afc8477e9f01a040bcdfdb82",
            "a4dab743e49749d7a85fe2987117c6f2",
            "516dd201888f41049da134eb378cdfaa",
            "a2b376b84133477380f4e7463b9f0e90",
            "827c7ca6f0d444c5bc83be06bfe7d7a2"
          ]
        },
        "outputId": "2e75675b-a081-4962-a625-dcdb5e7a0712",
        "id": "R3hqEWvUcyBB"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4fa63da64fe4ef9a95983c33e81b3fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5858b0df306a450681349d2fd293cb7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6067747070564704a1517353e1d84202"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dca8f9e656a042bebb3ef4fe1a2449d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef4e65803bb44723b2333ee48e6d37b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad76b76193d54428b21df0a45f13cc03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f2c94e7d34840f2900a0ab79c77ac4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7f4d506ec334e2484a04375eb52ec3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25482ba69e274aec8bf2022971244f5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa1fc5a322d446dcbb8865891693db86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cb169c90e1d436da5e1632b00551936"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can either use Chroma or FAISS to create the [Vector Store](https://python.langchain.com/docs/modules/data_connection/vectorstores.html)."
      ],
      "metadata": {
        "id": "u86xXj9v8cPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Create the vectorized db with FAISS\n",
        "from langchain.vectorstores import FAISS\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)\n",
        "\n",
        "# Create the vectorized db with Chroma\n",
        "# from langchain.vectorstores import Chroma\n",
        "# db_web = Chroma.from_documents(chunked_web_doc, embeddings)"
      ],
      "metadata": {
        "id": "gA3xtLSQmh-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecdf3a4-952e-4706-f3d9-9ebd7d4c1611"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.56 s, sys: 34.7 ms, total: 3.6 s\n",
            "Wall time: 3.92 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Use RetrievalQA chain\n",
        "We instantiate a RetrievalQA chain from LangChain which takes in a retriever, LLM and a chain_type as the input arguments. When the QA chain receives a query, the retriever retrieves information relevent to the query from the vectore store.   The ``chain type = \"stuff\"`` method stuffs all the retrieved information into context and makes a call to the language model. The LLM then generates the text/response from the retrieved documents. [See information on Langchain Retriver](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa).\n",
        "\n",
        "**LLM prompt structure**\n",
        "\n",
        "We can also pass in the recommended prompt structue for Llama 2 for the QA. In this way, we'd be able to advise our LLM to only use the available context to answer our question. If it cannot find information relevant to our query in the context, it'll **NOT** make up an answer, rather, it would advise that it's unable to find relevant information in the context."
      ],
      "metadata": {
        "id": "agfSAbN_zvYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# use the recommended propt style for the LLAMA 2 LLM\n",
        "prompt_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
        "    # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
        "    # k defines how many documents are returned; defaults to 4.\n",
        "    # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
        "    # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
        "    retriever=db_web.as_retriever(), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"When was the solar system formed?\"\n",
        "result = Chain_web.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "id": "GxfNkw_PnItp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f67b7b-9a8f-4d29-86f1-fe8f89455f74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.15 s, sys: 127 ms, total: 2.28 s\n",
            "Wall time: 2.44 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'When was the solar system formed?',\n",
              " 'result': '\\nBased on the information provided, the Solar System was formed approximately 4.568 billion years ago.'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DezXfGwjCQv2",
        "outputId": "5a5aabbf-dcde-4c85-dd69-3f2b9c4a3e9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided, the Solar System was formed approximately 4.568 billion years\n",
            "ago.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"Explain in detail how the solar system was formed.\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfraMfszR6BU",
        "outputId": "54fa67ca-f78a-472f-f16e-5b4a76a7c530"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The solar system was formed 4.568 billion years ago from the gravitational collapse of a region\n",
            "within a large molecular cloud. This region was likely several light-years across and may have given\n",
            "birth to several stars. The cloud consisted mostly of hydrogen, with some helium, and small amounts\n",
            "of heavier elements fused by previous generations of stars. The gravitational collapse led to the\n",
            "formation of a protostar, which eventually became the Sun. The remaining material in the cloud\n",
            "condensed into small, rocky bodies called planetesimals, which collided and merged to form the\n",
            "planets we know today.\n",
            "CPU times: user 8.04 s, sys: 240 ms, total: 8.28 s\n",
            "Wall time: 8.69 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"Why do the planets orbit the Sun in the same direction that the Sun is rotating?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pQu8anva0at",
        "outputId": "c8d8a450-dca7-4a4e-8729-4665bc8a6da1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided, the reason why the planets orbit the Sun in the same direction\n",
            "that the Sun is rotating is because they formed from a rotating disk of gas and dust that surrounded\n",
            "the young Sun. This disk was likely formed through the collapse of a giant cloud of gas and dust,\n",
            "and it rotated in the same direction as the Sun due to conservation of angular momentum. As the\n",
            "material in the disk cooled and condensed, it formed into the planets we see today, all orbiting the\n",
            "Sun in the same direction due to their shared origin and the conservation of angular momentum.\n",
            "CPU times: user 9.27 s, sys: 158 ms, total: 9.42 s\n",
            "Wall time: 10.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"What are the planets of the solar system composed of? Give a detailed response.\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0voKl7tcz-4",
        "outputId": "284cda63-5b16-43ce-e1fa-7ce2134f8a4c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the planets of the solar system are composed of the following:  1.\n",
            "Rocky materials: The four inner planets - Mercury, Venus, Earth, and Mars - are primarily composed\n",
            "of rocky materials. 2. Gaseous materials: The four giant planets - Jupiter, Saturn, Uranus, and\n",
            "Neptune - are primarily composed of gaseous materials, such as hydrogen and helium. 3. Icy\n",
            "materials: The Kuiper belt, which is located beyond the asteroid belt and consists of objects that\n",
            "are mostly icy, is thought to be the source of many short-period comets. 4. Dust and small\n",
            "particles: The solar system also contains a vast amount of dust and small particles that are found\n",
            "in the interplanetary space between the planets.  It is important to note that the composition of\n",
            "the planets can vary significantly depending on their location within the solar system. For example,\n",
            "the inner planets are much denser than the gas giants, and the gas giants have no solid surfaces due\n",
            "to their high temperatures and atmospheric pressures. Additionally, the Kuiper belt and other\n",
            "distant regions of the solar system are still largely unexplored, and there may be many more types\n",
            "of materials present in these areas that are not yet known.\n",
            "CPU times: user 17.3 s, sys: 277 ms, total: 17.6 s\n",
            "Wall time: 17.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Hallucination Check\n",
        "Hallucination in RAG refers to the generation of content by an LLM that is not based onn the retrieved knowledge.\n",
        "\n",
        "Let's test our LLM with a query that is not relevant to the context. The model should respond that it does not have enough information to respond to this query."
      ],
      "metadata": {
        "id": "L5l8ucgMjuLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"How does the tranformers architecture work?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWgQgd8FJcBm",
        "outputId": "125f42c8-018c-4ac6-99ed-6dfd27c4caa1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot provide a detailed answer to how the transformers architecture works based on the given\n",
            "context. The context only mentions \"ring systems\" and \"retrograde manner,\" which do not provide\n",
            "enough information to explain the transformers architecture. Therefore, I cannot answer the question\n",
            "with certainty.\n",
            "CPU times: user 3.27 s, sys: 106 ms, total: 3.38 s\n",
            "Wall time: 3.37 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model responded as expected. The context provided to it do not contain any information on tranformers architectures. So, it cannot answer this question!"
      ],
      "metadata": {
        "id": "-rwp6YEAkApG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG from PDF Files"
      ],
      "metadata": {
        "id": "JgSv0RrUPaDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download pdf files"
      ],
      "metadata": {
        "id": "JMoYqFnc4XMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://github.com/muntasirhsn/datasets/raw/main/Solar-System-Wikipedia.pdf\" # this is just a pdf print of the Solar System page on Wikipedia!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw_5w2c-PjeF",
        "outputId": "875c143b-ffdf-4a3c-8e3c-5410c466c471"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://github.com/muntasirhsn/datasets/raw/main/Solar-System-Wikipedia.pdf\n",
            "To: /content/Solar-System-Wikipedia.pdf\n",
            "\r  0% 0.00/4.49M [00:00<?, ?B/s]\r 35% 1.57M/4.49M [00:00<00:00, 13.6MB/s]\r100% 4.49M/4.49M [00:00<00:00, 20.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load PDF Files"
      ],
      "metadata": {
        "id": "Bzm9gKiFPoD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "pdf_loader = UnstructuredPDFLoader(\"/content/Solar-System-Wikipedia.pdf\")\n",
        "pdf_doc = pdf_loader.load()\n",
        "updated_pdf_doc = filter_complex_metadata(pdf_doc)"
      ],
      "metadata": {
        "id": "2O0JxHkUQsHg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spit the document into chunks"
      ],
      "metadata": {
        "id": "QEfY__rf4BjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
        "chunked_pdf_doc = text_splitter.split_documents(updated_pdf_doc)\n",
        "len(chunked_pdf_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--JjeRqGTwH-",
        "outputId": "c2515aa0-7cc7-4da0-f053-0602b7a78b95"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the vector store"
      ],
      "metadata": {
        "id": "s4s2c_Q547N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
      ],
      "metadata": {
        "id": "fXDUlIAsUsRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746d5df2-2355-4527-abb6-6736b4d6f798"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.62 s, sys: 7.29 ms, total: 5.63 s\n",
            "Wall time: 5.61 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG with RetrievalQA"
      ],
      "metadata": {
        "id": "I4Jx0m0e5J9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_pdf.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"When was the solar system formed?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-InFSJZVtTu",
        "outputId": "1b329152-0792-4db6-ec29-de72d52ff309"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the solar system was formed approximately 4.6 billion years ago. This\n",
            "information can be found in the second paragraph of the Wikipedia article, which states that the\n",
            "Solar System was formed \"over 4.6 billion years ago\" from the gravitational collapse of a giant\n",
            "molecular cloud.\n",
            "CPU times: user 6.39 s, sys: 848 ms, total: 7.24 s\n",
            "Wall time: 7.79 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"Explain in detail how the solar system was formed.\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isaD-BvlfdoD",
        "outputId": "5ad5b8c7-5b16-4916-aa4c-13734dde8e80"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, here is a detailed explanation of how the Solar System was formed:\n",
            "The Solar System was formed approximately 4.6 billion years ago from the gravitational collapse of a\n",
            "giant molecular cloud. This cloud was likely several light-years across and may have birthed several\n",
            "stars. The cloud consisted mostly of hydrogen, with some helium, and small amounts of heavier\n",
            "elements fused by previous generations of stars. As the region that would become the Solar System\n",
            "collapsed, conservation of angular momentum caused it to rotate faster, leading to the formation of\n",
            "a protoplanetary disk with a diameter of roughly 200 AU (30 billion km; 19 billion mi) and a hot,\n",
            "dense protostar at the center.  Over time, the contracting nebula continued to flatten into a disk,\n",
            "with the majority of the mass collecting at the center to form the Sun. As the material in the disk\n",
            "collided and coalesced, hundreds of protoplanets may have formed, but many of these objects either\n",
            "merged or were destroyed or ejected, leaving only the planets, dwarf planets, and other small bodies\n",
            "that we see today.  According to the Nice model, the final stages of the Solar System's formation\n",
            "involved the migration of the giant planets, including Jupiter, Saturn, Uranus, and Neptune, which\n",
            "likely underwent a process of orbital instability and then migrated to their current positions. This\n",
            "migration likely occurred due to interactions with the disk of gas and dust that surrounded the\n",
            "young Sun.  In conclusion, the Solar System was formed through the gravitational collapse of a giant\n",
            "molecular cloud, which led to the formation of a protoplanetary disk and the eventual coalescence of\n",
            "hundreds of protoplanets into the planets and other objects that we see today. The Nice model\n",
            "provides a framework for understanding the final stages of the Solar System's formation, including\n",
            "the migration of the giant planets to their current positions.\n",
            "CPU times: user 31.1 s, sys: 585 ms, total: 31.7 s\n",
            "Wall time: 35.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hallucination Check"
      ],
      "metadata": {
        "id": "4LWO2cgzgHS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"How does the tranformers architecture work?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgUFV7nti60t",
        "outputId": "e57397ce-024d-404e-fc7a-8ccada5b2792"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot answer your question because the provided text does not contain information about\n",
            "transformers architecture. The text discusses the structure and composition of the solar system, the\n",
            "Nice model, and the expansion of the Sun. It also mentions Christiaan Huygens and his discovery of\n",
            "Titan, as well as the observations of the transit of Venus in 1639 by Jeremiah Horrocks and William\n",
            "Crabtree. None of this information relates to transformers architecture. Therefore, I cannot provide\n",
            "an answer to your question based on the given context.\n",
            "CPU times: user 8.3 s, sys: 515 ms, total: 8.82 s\n",
            "Wall time: 9.06 s\n"
          ]
        }
      ]
    }
  ]
}