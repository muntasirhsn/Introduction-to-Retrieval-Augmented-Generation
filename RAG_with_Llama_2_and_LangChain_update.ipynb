{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",    
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG with Llama 2 and LangChain\n",
        "Retrieval-Augmented Generation (RAG) is a technique that combines a retriever and a generative language model to deliver accurate response. It involves retrieving relevant information from a large corpus and then generating contextually appropriate responses to queries. Here we use the quantized version of the Llama 2 13B LLM with LangChain to perform generative QA with RAG. The notebook file has been tested in Google Colab with T4 GPU. Please change the runtime type to T4 GPU before running the notebook."
      ],
      "metadata": {
        "id": "ce4-y4JJgVSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "tPpiHVgj4CmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.37.2 optimum==1.12.0 --quiet\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --quiet\n",
        "!pip install langchain==0.1.9 --quiet\n",
        "# !pip install chromadb\n",
        "!pip install sentence_transformers==2.4.0 --quiet\n",
        "!pip install unstructured --quiet\n",
        "!pip install pdf2image --quiet\n",
        "!pip install pdfminer.six==20221105 --quiet\n",
        "!pip install unstructured-inference --quiet\n",
        "!pip install faiss-gpu==1.7.2 --quiet\n",
        "!pip install pikepdf==8.13.0 --quiet\n",
        "!pip install pypdf==4.0.2 --quiet\n",
        "!pip install pillow_heif==0.15.0 --quiet"
      ],
      "metadata": {
        "id": "zjLkm65J_S0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77124bef-80e4-4fab-957a-639c201d0370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.6/380.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart Runtime"
      ],
      "metadata": {
        "id": "30AIKqF9rTuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Llama 2\n",
        "We will use the quantized version of the LLAMA 2 13B model from HuggingFace for our RAG task."
      ],
      "metadata": {
        "id": "X22Ccat1oXE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "model_name = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407,
          "referenced_widgets": [
            "a08079fb647e44a89b0bc118791216fe",
            "dd5b4f79ad084f32b7b1b6df49daef57",
            "3899900ebed14c9aac976be9a1e35c98",
            "afe9a8bf23754a6088c2c6f18a896f4b",
            "5c9241a4a2b6445ba3c7124a4b69496a",
            "95df4cd909b0452a954d81a671315563",
            "704e8500bc9f470197a5c0fe86382227",
            "7d721d7e5c6549b0860851399ff5d3d0",
            "39c830c318794dc7a6d332eb0e74b24f",
            "dbb21e63d8524fd2827790ff16b6e647",
            "4d42b613c7de4faea8906c2d2403adf3",
            "5935d7a65b0049c09763980cde45062f",
            "2208307dec43433992d1d403e396a2b6",
            "58ab9940c9f646a7973b1e8868bf52f4",
            "791fb3cee7e047eea2120c058a906099",
            "bea968e395904995a88897351c1fb3f5",
            "a9757e342a0144cd99740c7e20282754",
            "5cbac3b0b6c74003ac49b9374dbb158b",
            "95cdabf0c2ce4ec4b88fe6ae0e90db20",
            "20bbf3758ac346e3a2f8a1cbc91611d6",
            "b7a9d8a45f5a4efcae57ad785323e3cf",
            "1ec0ead6904a42e89eeb267665eb30df",
            "4ad07ae469aa49a993c4258a99a79414",
            "c9c4e09ec1444b3abf9ee8e46dabc4d3",
            "19cf5b38d7ad4725a833641626cbfdc3",
            "382d337325754cb79f762ab483b69e64",
            "5c1ce077dfed4fb082ba2844250c4ad6",
            "b7c8ef78e1a44dc799134bd001efb32d",
            "0d71675a1a654a4cb0386dd9cef0a9a7",
            "588f08ac683e43d6bb1f33eabe117607",
            "40675156400f43c285366bc89dea8ce1",
            "0f772992ce854b9c84be0dbe1cdff2c1",
            "3da5484d13a84f94a5a7106ac68ddd04",
            "07b931e61383495a97ae3f4d409377b0",
            "7c540c46a7334667a177bc133a28c943",
            "628933da42c84aa7af49893bee1039dc",
            "d55c89c9381442c3aa0dcdc14ee80f6a",
            "d00c401e07eb479b8c555180a5bef36d",
            "5f8238b2682d4ae588c071917834bc01",
            "da83bf6a56be47a29b42440c19229cf5",
            "f4d2e5152eaa44529dd7d4d3067c7332",
            "398c6a28eea54aa79911fc97daf9aae9",
            "ac5deb08c50a474cb071139a63792c07",
            "e1e0335f623c4c57b988cd8c7adee8e3",
            "5c309a298a63425cad0ba7e5d97fce5c",
            "fb82089f0edc44859ae4990d15ffd6e3",
            "b9eb6284311b4d4fbb12aba0be605430",
            "0bcb4f0b4d53452aaa2f4084765c0549",
            "0430bc64a7c342678b0f52765df49cb4",
            "ad3a187991024a00b2aaa20a25bc09d3",
            "de9a21cec2d943929693db2b0a653619",
            "b5d828b3aedd41bd8403033ab709c38e",
            "1d292becaa374f7eb118925244eb1381",
            "de969573638f417596375b434c76c410",
            "aeea885107824ba2b2e39f98913fd7c2",
            "027e9f11b0974a528912d93e37dfa653",
            "d132347a41b74968836c9e2bb484697b",
            "4053d9c9382e436db75e80fd94f87c53",
            "31f306a2fefd4656b76bbb88f14e25bf",
            "9c24b21c392c483c8a669a9c705ccafe",
            "67280289d450408392cb138c14825ede",
            "3a6817d75dfd4188b9c17d46d0ef8c28",
            "7b522c387ea442968889c211d6367588",
            "6021325870f24794bb101de768bda563",
            "bfc922e8d26e43c1ab9184ccd47dcdce",
            "efa1cd62ed4f4a8eb7cd62b2d4c8e498",
            "ed4da8beb0ec426e99fce074cc0fadde",
            "4d1c8aac9b6d461a99829961b0f27dcd",
            "e3ea6ffdbf9a458b9465aebbfdd98e02",
            "8339c95f25144a4e933deada603fbe0d",
            "370c87726d884bfebb70cc6db3f14024",
            "c445b85af39b4e7ea88f7d047b298627",
            "039f9a5a1b414253b6ba5830bc7a23f3",
            "d43811924795476a9404639c2f38f8a5",
            "85da0c8c598b4b6992d88bf1b73ffabc",
            "eb9cc001315c46bb9bef3758d2a60e9c",
            "8735c816904e467daf37be20aed06f9a"
          ]
        },
        "id": "N_hQ_IfuoWT6",
        "outputId": "bb05f544-2d2a-4e98-d1aa-cc649d9d8b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a08079fb647e44a89b0bc118791216fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5935d7a65b0049c09763980cde45062f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ad07ae469aa49a993c4258a99a79414"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07b931e61383495a97ae3f4d409377b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c309a298a63425cad0ba7e5d97fce5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "027e9f11b0974a528912d93e37dfa653"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed4da8beb0ec426e99fce074cc0fadde"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test LLM with Llama 2 prompt structure and LangChain PromptTemplate"
      ],
      "metadata": {
        "id": "R69LfWsnYAqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are an AI assistant. You are truthful, unbiased and honest in your response.\n",
        "\n",
        "If you are unsure about an answer, truthfully say \"I don't know\"\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "text = \"Explain artificial intelligence in a few lines\"\n",
        "result = llm.invoke(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "id": "BX6bnnL45a2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872d5715-a2e4-4426-d88b-3583b05f166b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here is my explanation of artificial intelligence:  Artificial intelligence (AI) refers to the\n",
            "development of computer systems that can perform tasks that typically require human intelligence,\n",
            "such as learning, problem-solving, and decision-making. These systems use algorithms and machine\n",
            "learning techniques to analyze data and make predictions or take actions based on that data. Some\n",
            "examples of AI include natural language processing, image recognition, and autonomous vehicles.\n",
            "Overall, the goal of AI is to create machines that can think and act like humans, but with greater\n",
            "speed, accuracy, and consistency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "M-BEF0Zgbsrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG from PDF Files\n",
        "### A. Create a vectore store for the context/external data\n",
        "Here, we'll create embedding vectores of the unstructured data loaded from the the source and store them in a vectore store.  "
      ],
      "metadata": {
        "id": "JgSv0RrUPaDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Download pdf files"
      ],
      "metadata": {
        "id": "JMoYqFnc4XMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://github.com/muntasirhsn/datasets/raw/main/Solar-System-Wikipedia.pdf\" # this is just a pdf print of the Solar System page on Wikipedia!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw_5w2c-PjeF",
        "outputId": "105d0023-5326-49ff-81ec-e778bf68d615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://github.com/muntasirhsn/datasets/raw/main/Solar-System-Wikipedia.pdf\n",
            "To: /content/Solar-System-Wikipedia.pdf\n",
            "\r  0% 0.00/4.49M [00:00<?, ?B/s]\r 58% 2.62M/4.49M [00:00<00:00, 25.9MB/s]\r100% 4.49M/4.49M [00:00<00:00, 33.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Load PDF Files\n",
        "Depending on the type of the source data, we can use the appropriate data loader from LangChain to load the data."
      ],
      "metadata": {
        "id": "Bzm9gKiFPoD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "\n",
        "pdf_loader = UnstructuredPDFLoader(\"/content/Solar-System-Wikipedia.pdf\")\n",
        "pdf_doc = pdf_loader.load()\n",
        "updated_pdf_doc = filter_complex_metadata(pdf_doc)"
      ],
      "metadata": {
        "id": "2O0JxHkUQsHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spit the document into chunks\n",
        "Due to the limited size of the context window of an LLM, the data need to be divided into smaller chunks with a text splitter like CharacterTextSplitter or RecursiveCharacterTextSplitter. In this way, the smaller chunks can be fed into the LLM."
      ],
      "metadata": {
        "id": "QEfY__rf4BjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
        "chunked_pdf_doc = text_splitter.split_documents(updated_pdf_doc)\n",
        "len(chunked_pdf_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--JjeRqGTwH-",
        "outputId": "6ef9d18f-603a-4958-ec59-7ff15cab5ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a vector database of the chunked documents with HuggingFace embeddings"
      ],
      "metadata": {
        "id": "s4s2c_Q547N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "3814f965c8dc4cba9045506edc23b0a5",
            "b7dce8a990084b8c9128ddfd95e7acd2",
            "b7563f25e48047a6ac05c3eebbe9e503",
            "73e971b567814f40b0107f34a3a635ce",
            "8f287f9f30ce4e94a30de55c205e98b4",
            "f977165401024d868df48bc48d7df945",
            "e5c42ffe1ac741af80a7dc76226789af",
            "b1825ce0cd4c4b41b24c25da4a1c9889",
            "a2b348454c3647889b90d95f64f475ce",
            "35e30b74b231420a804a7afa9abe1950",
            "832b91622831427ba95617c8252c0ba2",
            "79cbeba40c77496a976ef6ccd90156d0",
            "ed903d41b9bc498988658a0a5583c775",
            "50f9a51143304a0eacec56c6bd1b22d6",
            "793a3704e35f48be88e347d898f2afdf",
            "479ada8759794b73b5430aa3eb6619b9",
            "5d1f75628ad94e78a21f59486b2dd065",
            "d54774d9ab9546138cdb23a72b8c417b",
            "63a52f8affb2420b8754f716c273e9b5",
            "2cba2ba25dd94de3af3b6bb030c3b53b",
            "6bc708d0218045529a2e64f1a5910ad7",
            "6b4d6a4fe281458cb7b35d827eb6cea5",
            "02304080c0c14cfbba913637741d8ba6",
            "121eedc606bd401ea6c0c09bb310cd7a",
            "c70eb193f0b94df2a065abf07977cbd8",
            "9b0322169aed41a69e91ce37b49666d2",
            "68975cbf8e594f83ae5572708af1098f",
            "6baebb0c52ea4f0a866358b642cd5211",
            "ebba7a11643849a1827ec118db7486c7",
            "81c3c2b752a2442b9691fd171dbdd65a",
            "387ade3f62de4640b3b5c7aa9560180d",
            "5cd36b6390b14e62be926745181143b2",
            "f32dd73da7d6481292542e33cf6a0b66",
            "bea43cab7c5c49078104664b01d63ff8",
            "f1568522ea9947c6b62ea29d0dc25076",
            "33ae6c8c161c4df38c8eca9dac400258",
            "2d0d32c6a630459095f289e7a730b607",
            "3c73e26d543e4d3eb0ae9f2ff1bc355e",
            "4db802b083244b60bfabf756ce95e752",
            "f73da1d5938d4abeac74e236cc8b48b5",
            "ed1f0a6064a84e80b07bceec29a203a2",
            "fb538a9635b142a49098cc0389be01eb",
            "1d087410bd3946cfa5dfc38177cc554e",
            "e3cb5c86561b47d287abbc06f11ae1bd",
            "5213ca4220e54548b7b86d6267cc46b2",
            "7c74b9746a68445b961423da0b134f92",
            "5eec797e719c4a968f380240274c128b",
            "3d3ec53e790d416ba30d4148827a0fac",
            "7f523e7eb3fc49ce9571c0b3a365cbbd",
            "4e706f11781b420685793a510bb42269",
            "0d3ba59ea78e4399bfc959f36bac1abd",
            "f37835af2a4a43b087773625c8eb4d64",
            "170912e61327417c85c1426834a11838",
            "44c7a55f3aca41809bfc942916617d9c",
            "a67e6f08b36847038f753d4535d238bb",
            "beeebd05ba014e5395676deebafaab0b",
            "333fe20f2ad346ffacd44a7f744745ce",
            "aea5b13456be4296a7c1373769826216",
            "f3c46efc54174f7a8c7a41a8184f5403",
            "af6613ce94904f89b5dfae2d7aaabd88",
            "0b1e51845bbf42c0882b3f7df1a7c2c5",
            "af4d74b2defb47cc97547d6524993663",
            "7e5612d55d6b44628e0cc4ed40e90cc7",
            "d79161814d4e4d58a76f962b2fd29390",
            "34fe5de421b049b6b51f8ca3324b0610",
            "7c2c48b646004ecc80650eed361c118f",
            "31a0e37700434b3d8430f5e6db9ae7e6",
            "3f1386dc032e42069bfeebfdc8c0a1ff",
            "7b419c9003e84124b5e017e939274c37",
            "1f48ebb12ddd450a9710a787ccb84202",
            "d686557d020141acab5b557a9be50844",
            "7c4baf5356e945f3bb015ca1e1804c41",
            "05445cc7748d400596ce44b3511b7680",
            "cee6aea2ba15477c96d106580a163980",
            "4082cf1f830b4811953bd086f7d885cc",
            "232a8c002fe846ff98fc3353f956946a",
            "d07fdf84577446528b1d3e0d1a0c6d24",
            "92667c9f044d40459a334d9e2055178a",
            "02f58a27b0c64b1ab856821a36eb882a",
            "78801793526d4f6cbd9ffba83d23906c",
            "c14526fe51624b8895792af818c4183f",
            "2db948612e92492ca0926cbcbfd9aba8",
            "18e1229f11ee412588aee702f0fb7cda",
            "c01dcce8ff9b4d3ab7154a8035e2b1e6",
            "a80b2cd82066425aaa295eb0216555aa",
            "8916392928974c178224944d27fb2e78",
            "e181308200ea4a2a89e566a1cafbebca",
            "b8cb96d69d524d868f3e69091b37528a",
            "a46d0ef838204c3abc1e3a7b23a929a6",
            "06e6a452e0404040a3cf26b4f6ca8143",
            "e3846d159f3748bcaae62e59012a38cd",
            "5bd3b99b3ed846bfb0364a035c75c4a0",
            "49c916148ea546258d85ce2796b5c8a3",
            "7766d87c169f4b8084e1ff6c4276bf5d",
            "55d1fe05134f477eab49103d7393c8ca",
            "954f18886710490bb231bb7718faf39b",
            "b9a8032a8be848899bf685662bb45a0e",
            "bdb9107257834950bcf6a7bb7a01a643",
            "5f5af2f2201e4e17b9918b9e1c5b6ed9",
            "dbcc30d9058c43bda784ea1c8d5bc406",
            "dfd10b9762084565974488c1b2c3c697",
            "a520d154de5b4b9bbc6ad46711c8aebd",
            "c3c872b6770e412fa8bbe018eae58df6",
            "42ead63b93ef4e7b95d5c73786bf31a3",
            "ff1e200df82f4e2690fc11d5c7286cd1",
            "66b1061c2c224998b87697cfbba4d7c7",
            "d0b0545f3d7844cfb5691401f0fc2ab7",
            "79d46bcf4a034d9f96fee55849bf6720",
            "fdd028d08d88433cae7fd15eb8588547",
            "2d4296c8b095413a970a53edb376df9a",
            "49c5b956f0624adca6b9cbca5718502f",
            "517aec74bf4c40d89549b55802ca569c",
            "5f19e67897fb4001afdac650d9657def",
            "a4dfae4611804b98ab80a16bf9353f13",
            "ade97376a6fc4b358e770cff62cc5e10",
            "c9031e13d676472caba4f3f54b2717f2",
            "4d0df81f0c7b44268a0bffe434f265fd",
            "0a45e41607144cab8dbba460865423b3",
            "64920e6130be44eaaef4a8585cf831ca",
            "df9c20f532e943d7882d7f0d03305d21",
            "8548b7e0397b4d27a4659fa56d8853cb"
          ]
        },
        "outputId": "9c9febcc-f2fd-422f-c30a-67e5018f0e36",
        "id": "R3hqEWvUcyBB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3814f965c8dc4cba9045506edc23b0a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79cbeba40c77496a976ef6ccd90156d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02304080c0c14cfbba913637741d8ba6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bea43cab7c5c49078104664b01d63ff8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5213ca4220e54548b7b86d6267cc46b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "beeebd05ba014e5395676deebafaab0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31a0e37700434b3d8430f5e6db9ae7e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92667c9f044d40459a334d9e2055178a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a46d0ef838204c3abc1e3a7b23a929a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbcc30d9058c43bda784ea1c8d5bc406"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49c5b956f0624adca6b9cbca5718502f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can either use FAISS or Chroma to create the [Vector Store](https://python.langchain.com/docs/modules/data_connection/vectorstores.html)."
      ],
      "metadata": {
        "id": "u86xXj9v8cPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Create the vectorized db with FAISS\n",
        "from langchain.vectorstores import FAISS\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)\n",
        "\n",
        "# Create the vectorized db with Chroma\n",
        "# from langchain.vectorstores import Chroma\n",
        "# db_pdf = Chroma.from_documents(chunked_pdf_doc, embeddings)"
      ],
      "metadata": {
        "id": "fXDUlIAsUsRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5251e7a7-fe09-4e91-c546-2aa1736d9f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.21 s, sys: 18.3 ms, total: 5.23 s\n",
            "Wall time: 5.47 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Use RetrievalQA chain\n",
        "We instantiate a RetrievalQA chain from LangChain which takes in a retriever, LLM and a chain_type as the input arguments. When the QA chain receives a query, the retriever retrieves information relevent to the query from the vectore store.   The ``chain type = \"stuff\"`` method stuffs all the retrieved information into context and makes a call to the language model. The LLM then generates the text/response from the retrieved documents. [See information on Langchain Retriver](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa).\n",
        "\n",
        "**LLM prompt structure**\n",
        "\n",
        "We can also pass in the recommended prompt structue for Llama 2 for the QA. In this way, we'd be able to advise our LLM to only use the available context to answer our question. If it cannot find information relevant to our query in the context, it'll **NOT** make up an answer, rather, it would advise that it's unable to find relevant information in the context."
      ],
      "metadata": {
        "id": "agfSAbN_zvYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# use the recommended propt style for the LLAMA 2 LLM\n",
        "prompt_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
        "    # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
        "    # k defines how many documents are returned; defaults to 4.\n",
        "    # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
        "    # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
        "    retriever=db_pdf.as_retriever(), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"When was the solar system formed?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEmLGKwHHPy3",
        "outputId": "9891fed9-99b1-4f86-aa55-b62399994057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the solar system was formed approximately 4.6 billion years ago. This\n",
            "information can be found in the second paragraph of the Wikipedia article, which states that the\n",
            "Solar System was formed \"over 4.6 billion years ago\" from the gravitational collapse of a giant\n",
            "molecular cloud.\n",
            "CPU times: user 1min 5s, sys: 43.3 s, total: 1min 48s\n",
            "Wall time: 1min 48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"Explain in detail how the solar system was formed.\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isaD-BvlfdoD",
        "outputId": "6529cf38-b66a-41d4-c678-4af69ed3e241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, here is a detailed explanation of how the solar system was formed:\n",
            "The solar system was formed approximately 4.6 billion years ago from the gravitational collapse of a\n",
            "giant molecular cloud. This cloud was likely several light-years across and may have birthed several\n",
            "stars. The cloud consisted mostly of hydrogen, with some helium, and small amounts of heavier\n",
            "elements fused by previous generations of stars.  As the region that would become the solar system\n",
            "collapsed, conservation of angular momentum caused it to rotate faster. The center of the collapsing\n",
            "cloud became increasingly dense and hot, eventually forming a protostar at the center of the solar\n",
            "system. This protostar was surrounded by a protoplanetary disk, which gradually coalesced to form\n",
            "planets and other objects.  Hundreds of protoplanets may have existed in the early solar system, but\n",
            "they either merged or were destroyed or ejected, leaving the planets, dwarf planets, and leftover\n",
            "minor bodies that we see today. The planets formed by accretion, with dust and gas gravitationally\n",
            "attracting each other to form ever larger bodies.  The solar system's initial conditions were\n",
            "determined by the Nice model, which suggests that the planets formed in a chaotic dance before\n",
            "settling into their current orbits. This model proposes that the planets interacted with each other\n",
            "through gravity, causing their orbits to change over time.  In conclusion, the solar system was\n",
            "formed through the gravitational collapse of a giant molecular cloud, resulting in the formation of\n",
            "a protostar and a protoplanetary disk that coalesced to form planets and other objects. The initial\n",
            "conditions of the solar system were determined by the Nice model, which suggests that the planets\n",
            "formed in a chaotic dance before settling into their current orbits.\n",
            "CPU times: user 6min 10s, sys: 4min 9s, total: 10min 19s\n",
            "Wall time: 10min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"What are the planets of the solar system composed of? Give a detailed response.\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c99b11-f9aa-49e6-aa2a-9635b1092137",
        "id": "lJjevJWeDH8F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the planets of the solar system are composed of a variety of\n",
            "materials, including rocks, metals, gases, and ices.  The four terrestrial planets - Mercury, Venus,\n",
            "Earth, and Mars - are composed primarily of rocky materials, such as silicates and metals. These\n",
            "planets are also composed of iron and nickel, and have a definite surface.  The four giant planets -\n",
            "Jupiter, Saturn, Uranus, and Neptune - are composed mostly of gases, with extremely low melting\n",
            "points and high vapor pressure. These gases include hydrogen, helium, and neon. In addition, these\n",
            "planets have large amounts of ices, such as water, methane, ammonia, hydrogen sulfide, and carbon\n",
            "dioxide. These ices can be found as solids, liquids, or gases throughout the Solar System.  The\n",
            "composition of the planets in the Solar System is important because it affects their properties and\n",
            "characteristics. For example, the high metallicity of the Sun is thought to have played a role in\n",
            "the formation of its planetary system, as metals are necessary for the accretion of planetesimals.\n",
            "Additionally, the presence of ices in the outer Solar System is believed to have influenced the\n",
            "formation of the Kuiper belt and the outer reaches of the Solar System.\n",
            "CPU times: user 4min 47s, sys: 3min 14s, total: 8min 1s\n",
            "Wall time: 8min 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Hallucination Check\n",
        "Hallucination in RAG refers to the generation of content by an LLM that is not based onn the retrieved knowledge.\n",
        "\n",
        "Let's test our LLM with a query that is not relevant to the context. The model should respond that it does not have enough information to respond to this query."
      ],
      "metadata": {
        "id": "L5l8ucgMjuLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"How does the tranformers architecture work?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgUFV7nti60t",
        "outputId": "f8a01f6f-7ff6-4b18-f517-b58a5baa8052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot answer your question because the provided text does not contain information about\n",
            "transformers architecture. The text discusses the structure and composition of the solar system, the\n",
            "Nice model, and the expansion of the Sun. It also mentions Christiaan Huygens and his discovery of\n",
            "Titan, as well as the observations of the transit of Venus in 1639 by Jeremiah Horrocks and William\n",
            "Crabtree. None of this information relates to transformers architecture. Therefore, I cannot provide\n",
            "an answer to your question based on the given context.\n",
            "CPU times: user 1min 48s, sys: 1min 14s, total: 3min 3s\n",
            "Wall time: 3min 3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model responded as expected. The context provided to it do not contain any information on tranformers architectures. So, it cannot answer this question and do not suffer from hallucination!"
      ],
      "metadata": {
        "id": "bB1v_fNQGpRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG from web pages"
      ],
      "metadata": {
        "id": "a2SNJZEtlDo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Load the document\n",
        "\n"
      ],
      "metadata": {
        "id": "ffMdNNqecUq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "web_loader = UnstructuredURLLoader(\n",
        "    urls=[\"https://en.wikipedia.org/wiki/Solar_System\"], mode=\"elements\", strategy=\"fast\",\n",
        "    )\n",
        "web_doc = web_loader.load()\n",
        "updated_web_doc = filter_complex_metadata(web_doc)"
      ],
      "metadata": {
        "id": "z8Y3YnRjJGEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Split the documents into chunks"
      ],
      "metadata": {
        "id": "5rS8wix4cnKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
        "chunked_web_doc = text_splitter.split_documents(updated_web_doc)\n",
        "len(chunked_web_doc)"
      ],
      "metadata": {
        "id": "GVjSIuPMmYvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31524ce2-086c-47fb-ea96-85a52f5d4502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1463"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a vector database of the chunked documents with HuggingFace embeddings"
      ],
      "metadata": {
        "id": "k03r4pKIcyAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Create the vectorized db with FAISS\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)"
      ],
      "metadata": {
        "id": "gA3xtLSQmh-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b9b86e-c12a-4d8a-f28e-9d9587656427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.81 s, sys: 27.9 ms, total: 3.84 s\n",
            "Wall time: 3.89 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RAG with RetrievalQA"
      ],
      "metadata": {
        "id": "I4Jx0m0e5J9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_web.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"When was the solar system formed?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "id": "GxfNkw_PnItp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa21d13-1853-40b3-cef8-a7395888d4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided, the Solar System was formed approximately 4.568 billion years\n",
            "ago.\n",
            "CPU times: user 23.4 s, sys: 15.4 s, total: 38.8 s\n",
            "Wall time: 40.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"Explain in detail how the solar system was formed.\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfraMfszR6BU",
        "outputId": "4c2e3cad-1f22-49fe-f322-906e81d647e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The solar system was formed 4.568 billion years ago from the gravitational collapse of a region\n",
            "within a large molecular cloud. This region was likely several light-years across and may have given\n",
            "birth to several stars. The cloud consisted mostly of hydrogen, with some helium, and small amounts\n",
            "of heavier elements fused by previous generations of stars. The gravitational collapse led to the\n",
            "formation of a protostar, which eventually became the Sun. The remaining material in the cloud\n",
            "condensed into small, rocky bodies called planetesimals, which collided and merged to form the\n",
            "planets we know today.\n",
            "CPU times: user 2min, sys: 1min 20s, total: 3min 20s\n",
            "Wall time: 3min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"What are the planets of the solar system composed of? Give a detailed response.\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0voKl7tcz-4",
        "outputId": "ac3baa71-d381-4b85-dba7-3739415a69f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the planets of the solar system are composed of the following:  1.\n",
            "Rocky asteroids: The inner Solar System, which includes the four terrestrial planets (Mercury,\n",
            "Venus, Earth, and Mars), is surrounded by a belt of mostly rocky asteroids. 2. Icy objects: The\n",
            "outer Solar System, beyond the asteroid belt, includes the four giant planets (Jupiter, Saturn,\n",
            "Uranus, and Neptune) and the Kuiper belt of mostly icy objects. 3. Gaseous matter: The planets are\n",
            "also composed of gaseous matter, such as hydrogen and helium, which makes up a significant portion\n",
            "of their atmospheres. 4. Other materials: Each planet also contains a variety of other materials,\n",
            "such as metals, silicates, and organic compounds, which are present in different forms and\n",
            "proportions depending on the planet.  It is important to note that the composition of the planets\n",
            "can vary greatly, and some planets have unique features that set them apart from others. For\n",
            "example, Jupiter is primarily composed of hydrogen and helium, while Earth has a solid core made of\n",
            "iron and nickel, surrounded by a mantle of silicate rocks and a thin crust.\n",
            "CPU times: user 4min 23s, sys: 2min 58s, total: 7min 21s\n",
            "Wall time: 7min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hallucination Check"
      ],
      "metadata": {
        "id": "4LWO2cgzgHS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"How does the tranformers architecture work?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWgQgd8FJcBm",
        "outputId": "7bd14f79-87c5-48ec-f0c2-c397cceadb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot provide a detailed answer to how the Transformers architecture works based on the given\n",
            "context. The context only mentions \"ring systems\" and \"retrograde manner,\" which do not provide\n",
            "enough information to explain the Transformers architecture. The Transformers architecture is a\n",
            "complex topic that requires a comprehensive understanding of deep learning and neural networks, and\n",
            "it is not possible to explain it fully within the scope of this context.\n",
            "CPU times: user 1min 21s, sys: 54.9 s, total: 2min 16s\n",
            "Wall time: 2min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model does not suffer from hallucination!"
      ],
      "metadata": {
        "id": "-rwp6YEAkApG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uxXVK_Q7IEqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
